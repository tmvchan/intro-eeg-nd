{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadbb89-6b9e-4662-9052-df2eb08e658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16245c70-d2a9-4b96-89e9-e970a286806f",
   "metadata": {},
   "source": [
    "# Sensory evoked potentials\n",
    "\n",
    "We will spend this lab session testing out a couple of evoked potentials for ourselves! As we go through this lab session, I encourage you to keep in mind a couple of questions:\n",
    "* What do we know about how the signal typically looks like?\n",
    "* What about our current Muse setup would make this signal potentially look different?\n",
    "\n",
    "Embedded in this notebook are questions that you will be asked to answer as part of your lab report. The assignment on Canvas should also provide a copy of these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551c73ed-c21e-49f6-820f-15fbbdb81403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.7.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Imports: these are the packages we'll need to run today's trials.\n",
    "import os\n",
    "from eegnb import generate_save_fn\n",
    "from eegnb.devices.eeg import EEG\n",
    "from eegnb.experiments.visual_ssvep import ssvep\n",
    "from eegnb.experiments.auditory_ssaep import ssaep\n",
    "from eegnb.experiments.visual_vep import vep\n",
    "\n",
    "# Some standard pythonic imports\n",
    "import os, numpy as np, pandas as pd\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# MNE functions\n",
    "from mne import Epochs,find_events\n",
    "from mne.time_frequency import Spectrum, tfr_morlet\n",
    "\n",
    "# EEG-Notebooks functions\n",
    "from eegnb.analysis.utils2 import load_data,plot_conditions, check_report\n",
    "from eegnb.datasets import fetch_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042a8c6-3b06-45e4-96be-6d90e533c96b",
   "metadata": {},
   "source": [
    "## Steady-state visual evoked potential (SSVEP)\n",
    "\n",
    "The SSVEP is a signal generated by repeatedly presenting a visual stimulus at a particular rate, so called because it involves the steady presentation of a stimulus and the response being one automatically generated (i.e. evoked) by a visual stimulus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd9367-6c39-46a0-b5c6-dfa46ec1f951",
   "metadata": {},
   "source": [
    "### Experiment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c00a3f-830b-4eb8-a74e-87240bb918b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "board_name = \"muse2_bfn\" # you'll keep this the same, this is the method we are telling the computer we want to record EEG\n",
    "experiment = \"visual_ssvep\" # this is going to change depending on the experiment that you are running (you'll see it changes below)\n",
    "subject_id = 0 # change when you are recording a different person\n",
    "session_nb = 0 # change when you are recording the same person but with a different condition\n",
    "record_duration = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777aeca0-6bdd-4b2a-bc33-c789e8cf98f4",
   "metadata": {},
   "source": [
    "After this set-up, we will want to make sure we're using the right inputs before starting the experiment. Here's one way of looking at the inputs, based on where our data will be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6358e7-6dbc-4d06-9f73-2230a8a36295",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_device = EEG(device=board_name, serial_num='Muse-B00E') # change 'Muse-4DID' to whatever the 4-digit ID on your Muse is!\n",
    "\n",
    "# Create save file name\n",
    "save_fn = generate_save_fn(board_name, experiment, subject_id, session_nb)\n",
    "print(save_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166145f-e681-47f6-bc35-cc7bcb24fd0f",
   "metadata": {},
   "source": [
    "If the subject and session are labeled with the right places, then that's where you'll find the data when the file is saved after the experiment runs. This is good to keep in mind for when you go find the data for analysis after.\n",
    "\n",
    "Also, make sure you run the above cell once before every time you want to record the experiment. You'll see that the recording file name is time-stamped - if you don't run the cell again, you'll save over your old file with a new recording!\n",
    "\n",
    "### Run experiment\n",
    "\n",
    "Before starting an experiment, you want to make sure your data aren't too noisy. One way of doing this is to run a signal quality check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac0d2a3-222d-4eb8-9f92-208d27f26a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_report(eeg_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87fef4d-f9e6-4dfd-9c47-37514d8856d5",
   "metadata": {},
   "source": [
    "If you want to continue signal quality checks after the first 5, just hit ENTER. It'll continue to the next 5. To stop, press c before ENTER.\n",
    "\n",
    "To start the experiment, just run the block of code below after running the previous blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cef1a-b773-4779-b6d0-617b406fed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssvep.present(duration=record_duration, eeg=eeg_device, save_fn=save_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832eeeb3-e791-427f-afe6-a2fd20c02b12",
   "metadata": {},
   "source": [
    "If you are running the experiment multiple times, whether that's multiple sessions for a person or with different people, make sure that you are first running the previous cells where you change subject/session and also where the file is saved - otherwise, the files you generate will end up in the wrong folders, or you'll save over your own file!\n",
    "\n",
    "### Predictions\n",
    "\n",
    "Before we move on, please answer the following questions: \n",
    "\n",
    "* We are going to be looking at data based on frequency spectrum. What do you think we'll be able to see when looking at frequency data for the experiment that you just did? \n",
    "\n",
    "### Visualize data\n",
    "\n",
    "We're going to look at the data to see how they come out in response to the stimuli we've presented. The first step is to load out the data that we've just recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a51a527-e302-4d8d-a42a-ab0d20eb06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "Creating RawArray with float64 data, n_channels=5, n_times=30700\n",
      "    Range : 0 ... 30699 =      0.000 ...   119.918 secs\n",
      "Ready.\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "Creating RawArray with float64 data, n_channels=5, n_times=90028\n",
      "    Range : 0 ... 90027 =      0.000 ...   351.668 secs\n",
      "Ready.\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "Creating RawArray with float64 data, n_channels=5, n_times=31312\n",
      "    Range : 0 ... 31311 =      0.000 ...   122.309 secs\n",
      "Ready.\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "['TP9', 'Fp1', 'Fp2', 'TP10', 'stim']\n",
      "Creating RawArray with float64 data, n_channels=5, n_times=42472\n",
      "    Range : 0 ... 42471 =      0.000 ...   165.902 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "eegnb_data_path = os.path.join(os.path.expanduser('~/'),'.eegnb', 'data')    \n",
    "\n",
    "# to load data from a specific subject and session:\n",
    "subject = 0\n",
    "session = 0\n",
    "# make sure that these numbers are correct, or it will run from the last time you used 'subject' and 'session', \n",
    "# including from above!\n",
    "\n",
    "raw = load_data(subject,session,\n",
    "                experiment='visual_ssvep', site='local', device_name='muse2_bfn',\n",
    "                data_dir = eegnb_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb114cd-68fe-4c03-b997-1d0ae2834b98",
   "metadata": {},
   "source": [
    "The number of files you've read out (each file readout will tell you its length in seconds and then say 'Ready.' after) should correspond to the number of recordings you've done in that session. If it doesn't, go back and check.\n",
    "\n",
    "We're going to do a couple of things here that we won't go into now, but will help the final visual look like what it does above. We'll talk more about this process when we spend time formally going through the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e1b5b-bdb9-4b66-be37-23a69380a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will chunk (epoch) the data into segments representing the data 500ms before to 4s after each stimulus.\n",
    "\n",
    "events = find_events(raw)\n",
    "event_id = {'30 Hz': 1, '20 Hz': 2}\n",
    "epochs = Epochs(raw, events=events, event_id=event_id,\n",
    "                tmin=-0.5, tmax=4, baseline=None, preload=True,\n",
    "                verbose=False, picks=[0, 1, 2, 3])\n",
    "print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19642916-cfcc-464a-9988-faa62bf36f64",
   "metadata": {},
   "source": [
    "We're looking for that sample drop % to be as low as possible. If yours is higher than 15%, check the raw number of trials that have come out. It could be that the original recording was a bit noisy, and that's okay.\n",
    "\n",
    "Make sure you have at least 30 trials (where it says '(number) events found' tells you the trial count) before trying to run the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077d55a-1a8f-4942-bea0-f766ff452353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can compare the PSD of epochs specifically during 20hz and 30hz stimulus presentation\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "psd1, freq1 = epochs['30 Hz'].compute_psd(method='welch', n_fft=1028, n_per_seg=256 * 3, picks='all').get_data(return_freqs=True)\n",
    "psd2, freq2 = epochs['20 Hz'].compute_psd(method='welch', n_fft=1028, n_per_seg=256 * 3, picks='all').get_data(return_freqs=True)\n",
    "#psd1, freq1 = psd_welch(epochs['30 Hz'], n_fft=1028, n_per_seg=256 * 3, picks='all')\n",
    "#psd2, freq2 = psd_welch(epochs['20 Hz'], n_fft=1028, n_per_seg=256 * 3, picks='all')\n",
    "psd1 = 10 * np.log10(psd1)\n",
    "psd2 = 10 * np.log10(psd2)\n",
    "\n",
    "psd1_mean = psd1.mean(0)\n",
    "\n",
    "psd2_mean = psd2.mean(0)\n",
    "\n",
    "ax.plot(freq1, psd1_mean[[0, 3], :].mean(0), color='b', label='30 Hz')\n",
    "ax.plot(freq2, psd2_mean[[0, 3], :].mean(0), color='r', label='20 Hz')\n",
    "\n",
    "ax.set_title('TP9 and TP10')\n",
    "\n",
    "ax.set_ylabel('Power Spectral Density (dB)')\n",
    "\n",
    "ax.set_xlim((2, 50))\n",
    "\n",
    "ax.set_xlabel('Frequency (Hz)')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# With this visualization we can clearly see distinct peaks at 30hz and 20hz in the PSD, corresponding to the frequency of the visual stimulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22060b5d-1a41-4f97-a969-85a6d53753d3",
   "metadata": {},
   "source": [
    "Your output should have some semblance of the figure below (semblance is loose, don't worry):\n",
    "\n",
    "![SSVEP](ssvep.png)\n",
    "\n",
    "Answer the following questions: \n",
    "\n",
    "* What similarities or diferences do you notice between your graph and the sample one? Why do you think they are present? \n",
    "\n",
    "Use the below cell to save your figure (it will save in the introeeg_labs folder that you are currently in, just make sure to upload or email to yourself to have a copy for future reference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c8461-c9ae-4311-856a-ee09cb430c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this command to save your files (replace [yourname] with your name so you know which file is yours):\n",
    "plt.savefig('[yourname]_ssvep.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893566be-df4c-4713-abc1-79d2d124f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look for SSVEPs in the spectrogram, which uses color to represent the power of frequencies in the EEG signal over time\n",
    "\n",
    "frequencies = np.logspace(1, 1.75, 60)\n",
    "tfr, itc = tfr_morlet(epochs['30 Hz'], freqs=frequencies,picks='all',\n",
    "                              n_cycles=15, return_itc=True)\n",
    "tfr.plot(picks=[0], baseline=(-0.5, -0.1), mode='logratio',\n",
    "                 title='30 Hz stim');\n",
    "\n",
    "tfr, itc = tfr_morlet(epochs['20 Hz'], freqs=frequencies,picks='all',\n",
    "                              n_cycles=15, return_itc=True)\n",
    "tfr.plot(picks=[0], baseline=(-0.5, -0.1), mode='logratio',\n",
    "                 title='20 Hz stim');\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# SSVEPs clearer with 20Hz than 30Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831dae5-094a-4d09-813f-5a6a68e60a01",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "* Run the visualization again once you've collected 100 trials, and then 200 trials. What is different between the first time you visualized the file and these new visualizations? What might you expect to be different and why?\n",
    "* Where do you think this signal is originating from? Is the signal stronger or weaker than you might expect, given what we're recording with? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc69c3-4a1f-45b5-99fd-d6bf373e957f",
   "metadata": {},
   "source": [
    "## Visual Evoked Potential (VEP)\n",
    "\n",
    "The visual evoked potential is seen when visual stimuli are presented to an observer. \n",
    "\n",
    "NOTE: You'll see that some of the cells are the same as the ones above, or very similar. They're technically the same code, but I'm just repeating it down here so you don't overwrite what's above. Also, this way you can just skip down to this section if you're only interested in running the VEP!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f523d-7728-45d6-b98b-31c7e2db2cf8",
   "metadata": {},
   "source": [
    "### Experiment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c2d0e-f915-4870-ada1-1dcc3a3cb223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables\n",
    "board_name = \"muse2_bfn\" # you'll keep this the same, this is the method we are telling the computer we want to record EEG\n",
    "experiment = \"visual_vep\" # this is going to change depending on the experiment that you are running (you'll see it changes below)\n",
    "subject_id = 1 # change when you are recording a different person\n",
    "session_nb = 1 # change when you are recording the same person but with a different condition\n",
    "record_duration = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3adb6b-d12e-48be-a642-1eb495858ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the connection to the EEG device and what the file name will be\n",
    "eeg_device = EEG(device=board_name, serial_num='Muse-B00E')\n",
    "\n",
    "# Create save file name\n",
    "save_fn = generate_save_fn(board_name, experiment, subject_id, session_nb)\n",
    "print(save_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2791d-9b71-456f-822e-598baed7a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking signal quality before continuing\n",
    "check_report(eeg_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adde50-cd9e-403f-9c98-a57a89bc6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vep.present(duration=record_duration, eeg=eeg_device, save_fn=save_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173a4b3-da22-4f35-8a21-b48392feb6cd",
   "metadata": {},
   "source": [
    "Before you continue, you'll want to run this task a couple of times. I'd suggest 3 recordings of 120 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18024c-0f9d-49a2-98d4-991eb44ef814",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize data\n",
    "\n",
    "This time, we will be visualizing the data as ERPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68422960-ca27-4dc9-9856-9ec00bc24cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eegnb_data_path = os.path.join(os.path.expanduser('~/'),'.eegnb', 'data')    \n",
    "\n",
    "# to load data from a specific subject and session:\n",
    "subject = 1\n",
    "session = 1\n",
    "# make sure that these numbers are correct, or it will run from the last time you used 'subject' and 'session', \n",
    "# including from above!\n",
    "\n",
    "raw = load_data(subject,session,\n",
    "                experiment='visual_vep', site='local', device_name='muse2_bfn',\n",
    "                data_dir = eegnb_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195236d6-3227-4ac9-943c-80a8332e4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.filter(1,20, method='iir')\n",
    "# raw.plot_psd(fmin=1, fmax=30); # visualize power spectrum after filtering\n",
    "\n",
    "# Create an array containing the timestamps and type of each stimulus (i.e. face or house)\n",
    "events = find_events(raw)\n",
    "event_id = {'Left': 1, 'Right': 2}\n",
    "\n",
    "# Create an MNE Epochs object representing all the epochs around stimulus presentation\n",
    "epochs = Epochs(raw, events=events, event_id=event_id, \n",
    "                tmin=-0.1, tmax=0.6, baseline=(-0.1, 0),\n",
    "                reject={'eeg': 75e-6}, preload=True, \n",
    "                verbose=False, picks=[0,1,2,3])\n",
    "print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59e58b-53aa-4343-921b-f9063e18014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = OrderedDict()\n",
    "conditions['Left'] = [1]\n",
    "conditions['Right'] = [2]\n",
    "\n",
    "fig, ax = plot_conditions(epochs, conditions=conditions, \n",
    "                          n_boot=1000, title='', ci=0,\n",
    "                          channel_order=[0,1,3,2],\n",
    "                          diff_waveform=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da5e2f-f8b8-498a-a0cb-0812dd2116af",
   "metadata": {},
   "source": [
    "Your graph will hopefully look something like this:\n",
    "\n",
    "![VEP](vep_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0f76a-24fc-4317-8108-136756126a81",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "* Describe the pattern of activity you observed. Do you see any canonical ERPs?\n",
    "* Do you see a separation in pattern related to the presentation of left and right stimuli? Why do you think this is?\n",
    "* Try comparing what happens when someone is paying attention to the stimuli versus not paying attention. How do you plan on keeping track of attention? Did it affect the ERPs you identified earlier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c2125-7e30-4f21-9100-49f3bdd0e12b",
   "metadata": {},
   "source": [
    "## Steady-state auditory evoked potential (SSAEP)\n",
    "\n",
    "The SSAEP (also known as the auditory steady state response, or ASSR) is generated when "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
